# Data Ethics & Society Reading Group 03-03-2021

## Meeting info

### Description

Bias “[uncountable, countable] the fact that the results of research or an experiment are not accurate because a particular factor has not been considered when collecting the information”, Oxford Learners Dictionary.

## Suggested Material

These are not pre-requisite reading material for attendance, but a compilation of material the team find interesting and would read if they themselves have unlimited time.

### YouTube Videos

- Arvind Narayan, [21 Definitions of Fairness](https://www.youtube.com/watch?v=jIXIuYdnyyk) (55 mins)
- [Word Embeddings, Bias in ML, Why You Don’t Like Math, & Why AI Needs You](https://www.youtube.com/watch?v=25nC0n9ERq4) and the [jupyter notebooks](https://github.com/fastai/word-embeddings-workshop) (2 hrs 8 mins)
- FAT* 2019 [Translation Tutorial: Challenges of incorporating algorithmic fairness](https://www.youtube.com/watch?v=UicKZv93SOY) and [slides](https://drive.google.com/file/d/1rUQkVS0NzSH3IEqZDsczSxBbhYHbjamN/view) (1 hr 35 mins)

### Books

- Lisa Gitelman (ed.), [Raw data is an oxymoron,(introduction)](https://mitpress.mit.edu/books/raw-data-oxymoron) (192 pages) (not free)

### Blogs

- Moritz Hardt, [How big data is unfair](https://medium.com/@mrtz/how-big-data-is-unfair-9aa544d739de) (5 min read)

### Academic Papers

- Timnit Gebru et al, 2018. [Datasheets for Datasets](https://arxiv.org/abs/1803.09010) (24 pages)
- Birhane et al., 2020. [Values of Machine Learning](https://drive.google.com/file/d/1tjrm3Bf1hxV8iuPSiCcM1IazITGp-GZj/view) (6 pages)
- Harini Suresh and John Guttag, 2019. [A Framework for Understanding Unintended Consequences of Machine Learning](https://arxiv.org/abs/1901.10002) (10 pages)
- Samir Passi and Solon Barocas, 2019. [Problem Formulation and Fairness](https://arxiv.org/abs/1901.02547) (10 pages)
- Ulrich Aivodji et al, 2019. [Fairwashing: the risk of rationalization](https://arxiv.org/abs/1901.09749) (13 pages)
- Alice Xiang and Deborah Raji, 2019. [On the Legal Compatibility of Fairness Definitions](https://arxiv.org/abs/1912.00761) (6 pages)
- Grünewald and Pallas, 2020. [TILT: A GDPR-Aligned Transparency Information Language and Toolkit for Practical Privacy Engineering](https://arxiv.org/abs/2012.10431) (12 pages)
- Drosou et al., 2017. [Diversity in Big Data: A Review](https://pdfs.semanticscholar.org/7403/e177957cbb51f17018210da02d2ceab88f8a.pdf) (12 pages)
- Torralba and Efros, 2011. [Unbiased Look at Dataset Bias](http://www.wisdom.weizmann.ac.il/~vision/courses/2010_2/papers/datasets.pdf) (8 pages)

### Organisation and Government Reviews

- [CDEI Review into bias in algorithmic decision making](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/939109/CDEI_review_into_bias_in_algorithmic_decision-making.pdf) (147 pages)
- ["Discrimination, artificial intelligence and algorithmic decision-making" (commissioned by the Council of Europe’s European Commission Against Racism and Intolerance)](https://rm.coe.int/discrimination-artificial-intelligence-and-algorithmic-decision-making/1680925d73) (51 pages)

## Questions for discussion

### Related to  [Abeba Birhane 's paper on Algorithmic injustice: a relational ethics approach](https://www.sciencedirect.com/science/article/pii/S2666389921000155)

1. How can we leverage data practices in order to gain an in-depth understanding of certain problems as situated in structural inequalities and oppression (e.g. Institutional racism as defined by MacPherson Report)?”
2. How might a data worker engage vulnerable communities in ways that surface harms, when it is often the case that algorithmic harms may be secondary effects, invisible to designers and communities alike, and what questions might be asked to help anticipate these harms?”

### Related to [Launch of the Centre for Applied Data Ethics](https://uksa.statisticsauthority.gov.uk/publication/centre-for-applied-data-ethics-strategy-enabling-ethically-appropriate-research-and-statistics-for-the-public-good/)

1. What are the key challenges for data scientists in considering bias in their work?
2. Do we know enough about bias and how to prevent it in practice? Or are we still missing things?

## Related to [OSR review of approach to developing statistical models designed for awarding 2020 exam results](https://osr.statisticsauthority.gov.uk/our-regulatory-work/osr-review-of-approach-to-developing-statistical-models-designed-for-awarding-2020-exam-results/)

1. What could this mean for the models that you build or work with?
2. What historical patterns or assumptions could lead to the perception of bias?
3. How do you/ will you communicate those patterns and assumptions to users of the model so that they understand the impact on results?
